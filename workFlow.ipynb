{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code we are importing the required libraries for the training process including TensorFlow library as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of steps and the batch size of the training model are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'mnist_convnet'\n",
    "NUM_STEPS = 3000\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model_input function we are defining placeholders to give input to the variables after a certain time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(input_node_name, keep_prob_node_name):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28*28], name=input_node_name)\n",
    "    keep_prob = tf.placeholder(tf.float32, name=keep_prob_node_name)\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    return x, keep_prob, y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the build model function we go through with the following steps.\n",
    "-first we shape the image we get from the data set into a image with dimensions 28px X 28px.\n",
    "\n",
    "-next the given image is given to convolutional layer using a method called conv2d which is a part of the layers class. Here it takes three arguments (input, dimensionality of image in output space, An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window, *didn't find out what this does till now but there is another alternative valid* , activation function *we are using relu activation function* )\n",
    "\n",
    "-next the output of conv2d layer is given to the max_pooling layer which has the following arguments ( output of conv2d layer, a integer which denotes pool size, a integer which denotes stride size, and padding which is \"same or valid\")\n",
    "\n",
    "-we are using three layers each of which consists of a convolutional layer and a pooling layer. \n",
    "\n",
    "-then the image is flattened using the tf.reshape function which takes the following arguments. (input pooling layer, shape of the output *ranges from [[1,1,1,2,2,2,......],\n",
    "                            [..............4*4*256]]\n",
    "                            \n",
    "-the output of the reshape layer is given to the dense layer which constructs a dense layer. It takes the following arguments\n",
    "(output of flatten layer, no. of neurons, activation function)\n",
    "\n",
    "-To help improve the results of our model, we also apply dropout regularization to our dense layer, using the dropout method in layers.\n",
    "\n",
    "-The final layer in our neural network is the logits layer, which will return the raw values for our predictions. We create a dense layer with 10 neurons (one for each target class 0â€“9).\n",
    "\n",
    "-We can derive probabilities from our logits layer by applying softmax activation.\n",
    "\n",
    "-For both training and evaluation, we need to define a loss function that measures how closely the model's predictions match the target classes. \n",
    "\n",
    "-Adams optimizer has been used to optimize and the learning rate is 1e-4 i.e, 1*10^4. * The learning rate is how quickly a network abandons old beliefs for new ones. *\n",
    "\n",
    "-The accuracy is determined by comparing output of the dense layer with the testing dataset. The mean is found out and displayed to the user. \n",
    "\n",
    "-\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x, keep_prob, y_, output_node_name):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    # 28*28*1\n",
    "\n",
    "    conv1 = tf.layers.conv2d(x_image, 64, 3, 1, 'same', activation=tf.nn.relu)\n",
    "    #relu activation function-\n",
    "    # also called as rectifier linear unit activation function\n",
    "    # 28*28*64\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, 2, 2, 'same')\n",
    "    #layers of pooling \n",
    "    # 14*14*64\n",
    "\n",
    "    conv2 = tf.layers.conv2d(pool1, 128, 3, 1, 'same', activation=tf.nn.relu)\n",
    "    # 14*14*128\n",
    "    pool2 = tf.layers.max_pooling2d(conv2, 2, 2, 'same')\n",
    "    # 7*7*128\n",
    "\n",
    "    conv3 = tf.layers.conv2d(pool2, 256, 3, 1, 'same', activation=tf.nn.relu)\n",
    "    # 7*7*256\n",
    "    pool3 = tf.layers.max_pooling2d(conv3, 2, 2, 'same')\n",
    "    # 4*4*256\n",
    "\n",
    "    #flattenning the image\n",
    "    flatten = tf.reshape(pool3, [-1, 4*4*256])\n",
    "    fc = tf.layers.dense(flatten, 1024, activation=tf.nn.relu)\n",
    "    dropout = tf.nn.dropout(fc, keep_prob)\n",
    "    logits = tf.layers.dense(dropout, 10)\n",
    "    outputs = tf.nn.softmax(logits, name=output_node_name)\n",
    "\n",
    "    # loss\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits))\n",
    "\n",
    "    # train step\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(outputs, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    return train_step, loss, accuracy, merged_summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-The data is loaded into the neural network and the variables are initialized.\n",
    "\n",
    "-The tensor board is also initialized. \n",
    "\n",
    "-The tensor is trained according to the batch size recursively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, keep_prob, y_, train_step, loss, accuracy,\n",
    "        merged_summary_op, saver):\n",
    "    print(\"training start...\")\n",
    "\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        tf.train.write_graph(sess.graph_def, 'out',\n",
    "            MODEL_NAME + '.pbtxt', True)\n",
    "\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter('logs/',\n",
    "            graph=tf.get_default_graph())\n",
    "\n",
    "        for step in range(NUM_STEPS):\n",
    "            batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "            if step % 100 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={\n",
    "                    x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "                print('step %d, training accuracy %f' % (step, train_accuracy))\n",
    "            _, summary = sess.run([train_step, merged_summary_op],\n",
    "                feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "            summary_writer.add_summary(summary, step)\n",
    "\n",
    "        saver.save(sess, 'out/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "        test_accuracy = accuracy.eval(feed_dict={x: mnist.test.images,\n",
    "                                    y_: mnist.test.labels,\n",
    "                                    keep_prob: 1.0})\n",
    "        print('test accuracy %g' % test_accuracy)\n",
    "\n",
    "    print(\"training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-The trained model is exported and saved in the form of pb file which is later given to the android application as a library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(input_node_names, output_node_name):\n",
    "    freeze_graph.freeze_graph('out/' + MODEL_NAME + '.pbtxt', None, False,\n",
    "        'out/' + MODEL_NAME + '.chkp', output_node_name, \"save/restore_all\",\n",
    "        \"save/Const:0\", 'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    print(\"graph saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions are called from the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if not path.exists('out'):\n",
    "        os.mkdir('out')\n",
    "\n",
    "    input_node_name = 'input'\n",
    "    keep_prob_node_name = 'keep_prob'\n",
    "    output_node_name = 'output'\n",
    "\n",
    "    x, keep_prob, y_ = model_input(input_node_name, keep_prob_node_name)\n",
    "\n",
    "    train_step, loss, accuracy, merged_summary_op = build_model(x, keep_prob,\n",
    "        y_, output_node_name)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    train(x, keep_prob, y_, train_step, loss, accuracy,\n",
    "        merged_summary_op, saver)\n",
    "\n",
    "    export_model([input_node_name, keep_prob_node_name], output_node_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
